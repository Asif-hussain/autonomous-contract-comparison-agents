{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contract Comparison System - Testing Notebook\n",
    "\n",
    "This notebook demonstrates the complete contract comparison workflow using test contracts from `data/test_contracts/`.\n",
    "\n",
    "## Features Demonstrated:\n",
    "1. **Image Parsing**: Converting contract images to text using GPT-4o Vision\n",
    "2. **Two-Agent System**: Contextualization and extraction agents working together\n",
    "3. **Guardrails**: Input validation and safety checks\n",
    "4. **Evaluation**: Output quality assessment\n",
    "5. **Tracing**: Complete Langfuse observability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(project_root / '.env')\n",
    "\n",
    "# Import project modules\n",
    "from src.main import (\n",
    "    initialize_clients,\n",
    "    process_contract_comparison,\n",
    "    validate_environment\n",
    ")\n",
    "from src.image_parser import parse_contract_image\n",
    "from src.models import ContractChangeOutput, ParsedContract, AgentContext\n",
    "from src.agents.contextualization_agent import ContextualizationAgent\n",
    "from src.agents.extraction_agent import ExtractionAgent\n",
    "\n",
    "# Import guardrails and evaluator (we'll create these)\n",
    "try:\n",
    "    from src.guardrails import ContractGuardrails\n",
    "    from src.evaluator import ContractEvaluator\n",
    "    ENHANCED_MODE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️  Guardrails and Evaluator modules not found. Running in basic mode.\")\n",
    "    ENHANCED_MODE = False\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"✓ Enhanced mode: {ENHANCED_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment variables\n",
    "if not validate_environment():\n",
    "    raise ValueError(\"Environment validation failed. Check your .env file.\")\n",
    "\n",
    "print(\"✓ Environment validated\")\n",
    "print(f\"  OPENAI_API_KEY: {'✓' if os.getenv('OPENAI_API_KEY') else '✗'}\")\n",
    "print(f\"  LANGFUSE_PUBLIC_KEY: {'✓' if os.getenv('LANGFUSE_PUBLIC_KEY') else '✗'}\")\n",
    "print(f\"  LANGFUSE_SECRET_KEY: {'✓' if os.getenv('LANGFUSE_SECRET_KEY') else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI and Langfuse clients\n",
    "openai_client, langfuse_client = initialize_clients()\n",
    "\n",
    "print(\"✓ Clients initialized\")\n",
    "print(f\"  OpenAI client: {type(openai_client).__name__}\")\n",
    "print(f\"  Langfuse client: {type(langfuse_client).__name__}\")\n",
    "\n",
    "# Initialize guardrails and evaluator if available\n",
    "if ENHANCED_MODE:\n",
    "    guardrails = ContractGuardrails()\n",
    "    evaluator = ContractEvaluator()\n",
    "    print(\"✓ Guardrails and Evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover Test Contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all test contracts\n",
    "test_contracts_dir = project_root / 'data' / 'test_contracts'\n",
    "\n",
    "# Find pairs of original and amendment contracts\n",
    "contract_pairs = []\n",
    "\n",
    "for original_file in sorted(test_contracts_dir.glob('*_original.jpg')):\n",
    "    contract_num = original_file.stem.split('_')[0]\n",
    "    amendment_file = test_contracts_dir / f\"{contract_num}_amendment.jpg\"\n",
    "    \n",
    "    if amendment_file.exists():\n",
    "        contract_pairs.append({\n",
    "            'name': contract_num,\n",
    "            'original': str(original_file),\n",
    "            'amendment': str(amendment_file)\n",
    "        })\n",
    "\n",
    "print(f\"✓ Found {len(contract_pairs)} contract pair(s):\\n\")\n",
    "for i, pair in enumerate(contract_pairs, 1):\n",
    "    print(f\"  {i}. {pair['name']}\")\n",
    "    print(f\"     Original:  {Path(pair['original']).name}\")\n",
    "    print(f\"     Amendment: {Path(pair['amendment']).name}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Image Parsing\n",
    "\n",
    "Test the image parsing functionality with the first contract pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if contract_pairs:\n",
    "    test_pair = contract_pairs[0]\n",
    "    print(f\"Testing with: {test_pair['name']}\\n\")\n",
    "    \n",
    "    # Parse original contract\n",
    "    print(\"Parsing original contract...\")\n",
    "    original_contract = parse_contract_image(\n",
    "        image_path=test_pair['original'],\n",
    "        document_type=\"original\",\n",
    "        client=openai_client\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Original parsed:\")\n",
    "    print(f\"  Text length: {len(original_contract.raw_text)} characters\")\n",
    "    print(f\"  Sections: {len(original_contract.sections_identified)}\")\n",
    "    print(f\"  First 200 chars: {original_contract.raw_text[:200]}...\\n\")\n",
    "    \n",
    "    # Parse amendment contract\n",
    "    print(\"Parsing amendment contract...\")\n",
    "    amendment_contract = parse_contract_image(\n",
    "        image_path=test_pair['amendment'],\n",
    "        document_type=\"amendment\",\n",
    "        client=openai_client\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Amendment parsed:\")\n",
    "    print(f\"  Text length: {len(amendment_contract.raw_text)} characters\")\n",
    "    print(f\"  Sections: {len(amendment_contract.sections_identified)}\")\n",
    "    print(f\"  First 200 chars: {amendment_contract.raw_text[:200]}...\")\n",
    "else:\n",
    "    print(\"⚠️  No contract pairs found in data/test_contracts/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Guardrails (Input Validation)\n",
    "\n",
    "Apply guardrails to validate the parsed contracts before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENHANCED_MODE and contract_pairs:\n",
    "    print(\"Running guardrails checks...\\n\")\n",
    "    \n",
    "    # Validate original contract\n",
    "    original_validation = guardrails.validate_input(\n",
    "        contract=original_contract,\n",
    "        file_path=test_pair['original']\n",
    "    )\n",
    "    \n",
    "    print(\"Original Contract Validation:\")\n",
    "    print(f\"  Valid: {original_validation['is_valid']}\")\n",
    "    print(f\"  Checks passed: {original_validation['checks_passed']}/{original_validation['total_checks']}\")\n",
    "    if original_validation['warnings']:\n",
    "        print(f\"  Warnings: {original_validation['warnings']}\")\n",
    "    print()\n",
    "    \n",
    "    # Validate amendment contract\n",
    "    amendment_validation = guardrails.validate_input(\n",
    "        contract=amendment_contract,\n",
    "        file_path=test_pair['amendment']\n",
    "    )\n",
    "    \n",
    "    print(\"Amendment Contract Validation:\")\n",
    "    print(f\"  Valid: {amendment_validation['is_valid']}\")\n",
    "    print(f\"  Checks passed: {amendment_validation['checks_passed']}/{amendment_validation['total_checks']}\")\n",
    "    if amendment_validation['warnings']:\n",
    "        print(f\"  Warnings: {amendment_validation['warnings']}\")\n",
    "else:\n",
    "    print(\"⚠️  Guardrails not available or no contracts to validate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Two-Agent Workflow\n",
    "\n",
    "Test the complete two-agent system: contextualization and extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if contract_pairs:\n",
    "    print(\"Running two-agent workflow...\\n\")\n",
    "    \n",
    "    # Agent 1: Contextualization\n",
    "    print(\"Agent 1: Contextualization\")\n",
    "    agent1 = ContextualizationAgent(client=openai_client)\n",
    "    context = agent1.analyze(\n",
    "        original_contract=original_contract,\n",
    "        amendment_contract=amendment_contract\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Context generated:\")\n",
    "    print(f\"  Structure analysis: {len(context.document_structure)} chars\")\n",
    "    print(f\"  Section mappings: {len(context.corresponding_sections)}\")\n",
    "    print(f\"  Change areas identified: {len(context.identified_change_areas)}\")\n",
    "    print(f\"  Context summary: {context.context_summary[:150]}...\\n\")\n",
    "    \n",
    "    # Agent 2: Extraction\n",
    "    print(\"Agent 2: Change Extraction\")\n",
    "    agent2 = ExtractionAgent(client=openai_client)\n",
    "    changes = agent2.extract_changes(\n",
    "        original_contract=original_contract,\n",
    "        amendment_contract=amendment_contract,\n",
    "        context=context\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Changes extracted:\")\n",
    "    print(f\"  Sections changed: {len(changes.sections_changed)}\")\n",
    "    print(f\"  Topics touched: {len(changes.topics_touched)}\")\n",
    "    print(f\"  Summary length: {len(changes.summary_of_the_change)} chars\")\n",
    "else:\n",
    "    print(\"⚠️  No contracts to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Output Evaluation\n",
    "\n",
    "Evaluate the quality of the extracted changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENHANCED_MODE and contract_pairs:\n",
    "    print(\"Evaluating output quality...\\n\")\n",
    "    \n",
    "    evaluation = evaluator.evaluate_output(\n",
    "        changes=changes,\n",
    "        original_contract=original_contract,\n",
    "        amendment_contract=amendment_contract,\n",
    "        context=context\n",
    "    )\n",
    "    \n",
    "    print(\"Quality Evaluation:\")\n",
    "    print(f\"  Overall Score: {evaluation['overall_score']:.2f}/100\")\n",
    "    print(f\"  Grade: {evaluation['grade']}\\n\")\n",
    "    \n",
    "    print(\"Dimension Scores:\")\n",
    "    for dimension, score in evaluation['dimension_scores'].items():\n",
    "        print(f\"  {dimension.replace('_', ' ').title()}: {score:.2f}/100\")\n",
    "    \n",
    "    if evaluation['recommendations']:\n",
    "        print(\"\\nRecommendations:\")\n",
    "        for rec in evaluation['recommendations']:\n",
    "            print(f\"  - {rec}\")\n",
    "else:\n",
    "    print(\"⚠️  Evaluator not available or no output to evaluate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Complete Workflow with All Contracts\n",
    "\n",
    "Process all contract pairs through the complete workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, pair in enumerate(contract_pairs, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing Contract {i}/{len(contract_pairs)}: {pair['name']}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Run complete workflow\n",
    "        changes, trace_id = process_contract_comparison(\n",
    "            original_image_path=pair['original'],\n",
    "            amendment_image_path=pair['amendment'],\n",
    "            openai_client=openai_client\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'contract_name': pair['name'],\n",
    "            'trace_id': trace_id,\n",
    "            'changes': changes.model_dump(),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Add evaluation if available\n",
    "        if ENHANCED_MODE:\n",
    "            # Re-parse for evaluation\n",
    "            orig = parse_contract_image(pair['original'], 'original', openai_client)\n",
    "            amend = parse_contract_image(pair['amendment'], 'amendment', openai_client)\n",
    "            ctx = agent1.analyze(orig, amend)\n",
    "            \n",
    "            eval_result = evaluator.evaluate_output(changes, orig, amend, ctx)\n",
    "            result['evaluation'] = eval_result\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"\\n✓ Success!\")\n",
    "        print(f\"  Sections changed: {len(changes.sections_changed)}\")\n",
    "        print(f\"  Topics touched: {len(changes.topics_touched)}\")\n",
    "        if ENHANCED_MODE:\n",
    "            print(f\"  Quality score: {eval_result['overall_score']:.2f}/100\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error processing {pair['name']}: {str(e)}\")\n",
    "        results.append({\n",
    "            'contract_name': pair['name'],\n",
    "            'error': str(e),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Completed: {len(results)} contracts processed\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if results:\n",
    "    # Create summary dataframe\n",
    "    summary_data = []\n",
    "    for result in results:\n",
    "        if 'error' not in result:\n",
    "            summary_data.append({\n",
    "                'Contract': result['contract_name'],\n",
    "                'Sections Changed': len(result['changes']['sections_changed']),\n",
    "                'Topics Touched': len(result['changes']['topics_touched']),\n",
    "                'Summary Length': len(result['changes']['summary_of_the_change']),\n",
    "                'Quality Score': f\"{result['evaluation']['overall_score']:.1f}\" if ENHANCED_MODE and 'evaluation' in result else 'N/A',\n",
    "                'Trace ID': result['trace_id'][:8] + '...' if result['trace_id'] else 'N/A'\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(summary_data)\n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed View: First Contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and 'error' not in results[0]:\n",
    "    print(f\"\\nDetailed Results for {results[0]['contract_name']}:\\n\")\n",
    "    \n",
    "    changes = results[0]['changes']\n",
    "    \n",
    "    print(\"SECTIONS CHANGED:\")\n",
    "    for i, section in enumerate(changes['sections_changed'], 1):\n",
    "        print(f\"  {i}. {section}\")\n",
    "    \n",
    "    print(\"\\nTOPICS TOUCHED:\")\n",
    "    for i, topic in enumerate(changes['topics_touched'], 1):\n",
    "        print(f\"  {i}. {topic}\")\n",
    "    \n",
    "    print(\"\\nSUMMARY OF CHANGES:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(changes['summary_of_the_change'])\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if ENHANCED_MODE and 'evaluation' in results[0]:\n",
    "        print(\"\\nQUALITY EVALUATION:\")\n",
    "        eval_data = results[0]['evaluation']\n",
    "        print(f\"  Overall Score: {eval_data['overall_score']:.2f}/100\")\n",
    "        print(f\"  Grade: {eval_data['grade']}\")\n",
    "        print(\"\\n  Dimension Scores:\")\n",
    "        for dim, score in eval_data['dimension_scores'].items():\n",
    "            print(f\"    {dim.replace('_', ' ').title()}: {score:.2f}\")\n",
    "else:\n",
    "    print(\"No successful results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    output_dir = project_root / 'notebooks' / 'outputs'\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_file = output_dir / f'test_results_{timestamp}.json'\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✓ Results saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"No results to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Flush Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush Langfuse traces\n",
    "langfuse_client.flush()\n",
    "print(\"✓ Langfuse traces flushed\")\n",
    "print(f\"\\nView traces at: {os.getenv('LANGFUSE_HOST', 'https://cloud.langfuse.com')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
